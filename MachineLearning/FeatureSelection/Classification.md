# Feature selection for classification

## Boxplots and Violin plots

```python
import pandas as pd
import seaborn as sns
sns.set()
df = pd.DataFrame(data.data,columns=data.feature_names)
df['target'] = data.target
df_temp = pd.melt(df,id_vars='target',value_vars=list(df.columns)[:-1], 
                  var_name="Feature", value_name="Value")
g = sns.FacetGrid(data = df_temp, col="Feature", col_wrap=4, size=4.5,sharey = False)
g.map(sns.boxplot,"target", "Value");
g = sns.FacetGrid(data = df_temp, col="Feature", col_wrap=4, size=4.5,sharey = False)
g.map(sns.violinplot,"target", "Value")
```

## Summary table

å„ç§ç‰¹å¾ç­›é€‰æ±‡æ€»
![summary table](pics/summaryTable.png)

## Unsupervised methods

PCA

```python
from sklearn.datasets import load_iris
from sklearn.decomposition import PCA
from sklearn.svm import SVC
import matplotlib.pyplot as plt
from matplotlib.colors import ListedColormap
import numpy as np
h = .01
x_min, x_max = -4,4
y_min, y_max = -1.5,1.5
# loading dataset
data = load_iris()
X, y = data.data, data.target
# selecting first 2 components of PCA
X_pca = PCA().fit_transform(X)
X_selected = X_pca[:,:2]
# training classifier and evaluating on the whole plane
clf = SVC(kernel='linear')
clf.fit(X_selected,y)
xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
                     np.arange(y_min, y_max, h))
Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])
Z = Z.reshape(xx.shape)
# Plotting
cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF'])
cmap_bold = ListedColormap(['#FF0000', '#00FF00', '#0000FF'])
plt.figure(figsize=(10,5))
plt.pcolormesh(xx, yy, Z, alpha=.6,cmap=cmap_light)
plt.title('PCA - Iris dataset')
plt.xlabel('Dimension 1')
plt.ylabel('Dimension 2')
plt.scatter(X_pca[:,0],X_pca[:,1],c=data.target,cmap=cmap_bold)
plt.show()
```

![pca](pics/pca.png)


## Univariate filter methods

Filter methods aim at ranking the importance of the features without making use of any type of classification algorithm.

Univariate filter methods evaluate each feature individually and do not consider feature interactions. These methods consist of providing a score to each feature, often based on statistical tests.

The scores usually either measure the dependency between the dependent variable and the features (e.g. Chi2 and, for regression, Pearls correlation coefficient), or the difference between the distributions of the features given the class label (F-test and T-test).

The scores often make assumptions about the statistical properties of the underlying data. Understanding these assumptions is important to decide which test to use, even though some of them are robust to violations of the assumptions.

Scores based on statistical tests provide a p-value, that may be used to rule out some features. This is done if the p-value is above a certain threshold (typically 0.01 or 0.05).

Common tests include:


The package sklearn implements some filter methods. However, as most of them are based on statistical tests, statistics packages (such asstatsmodels) could also be used.

One example is shown below:
```python
from sklearn.feature_selection import f_classif, chi2, mutual_info_classif
from statsmodels.stats.multicomp import pairwise_tukeyhsd
from sklearn.datasets import load_iris
data = load_iris()
X,y = data.data, data.target
chi2_score, chi_2_p_value = chi2(X,y)
f_score, f_p_value = f_classif(X,y)
mut_info_score = mutual_info_classif(X,y)
pairwise_tukeyhsd = [list(pairwise_tukeyhsd(X[:,i],y).reject) for i in range(4)]
print('chi2 score        ', chi2_score)
print('chi2 p-value      ', chi_2_p_value)
print('F - score score   ', f_score)
print('F - score p-value ', f_p_value)
print('mutual info       ', mut_info_score)
print('pairwise_tukeyhsd',pairwise_tukeyhsd)

# -----------------------------------------------

Out:
chi2 score         [ 10.82   3.71 116.31  67.05]
chi2 p-value       [0.   0.16 0.   0.  ]
F - score score    [ 119.26   49.16 1180.16  960.01]
F - score p-value  [0. 0. 0. 0.]
mutual info        [0.51 0.27 0.98 0.98]
pairwise_tukeyhsd [[True, True, True], [True, True, True], [True, True, True], [True, True, True]]
```


## Multivariate Filter methods

è¿™ç±»æ–¹æ³•è€ƒè™‘äº†å˜é‡ä¹‹é—´çš„ç›¸å…³æ€§ï¼Œå¹¶ä¸”æ²¡æœ‰è€ƒè™‘ä»»ä½•åˆ†ç±»ç®—æ³•ã€‚

### mRMR

æœ€å°åŒ–å†—ä½™æœ€å¤§åŒ–ç›¸å…³æ€§
mRMR (minimum Redundancy Maximum Relevance) is a heuristic algorithm to find a close to optimal subset of features by considering both the features importances and the correlations between them.

è¿™ä¸ªæƒ³æ³•æ˜¯ï¼Œå³ä½¿ä¸¤ä¸ªç‰¹å¾é«˜åº¦ç›¸å…³ï¼Œå¦‚æœå®ƒä»¬é«˜åº¦ç›¸å…³ï¼Œå°†å®ƒä»¬éƒ½æ·»åŠ åˆ°ç‰¹å¾é›†ä¸­å¯èƒ½ä¸æ˜¯ä¸€ä¸ªå¥½ä¸»æ„ã€‚ åœ¨é‚£ç§æƒ…å†µä¸‹ï¼Œæ·»åŠ è¿™ä¸¤ä¸ªç‰¹å¾ä¼šå¢åŠ æ¨¡å‹çš„å¤æ‚æ€§ï¼ˆå¢åŠ è¿‡åº¦æ‹Ÿåˆçš„å¯èƒ½æ€§ï¼‰ä½†ä¸ä¼šå¢åŠ é‡è¦ä¿¡æ¯ï¼Œå› ä¸ºç‰¹å¾ä¹‹é—´å­˜åœ¨ç›¸å…³æ€§ã€‚

åœ¨åŒ…å« N ä¸ªç‰¹å¾çš„é›†åˆ S ä¸­ï¼Œç‰¹å¾ (D) çš„ç›¸å…³æ€§è®¡ç®—å¦‚ä¸‹ï¼š
![D-equation](pics/D-equation.png)

å…¶ä¸­ I æ˜¯äº’ä¿¡æ¯ç®—å­ã€‚

ç‰¹å¾çš„å†—ä½™è¡¨ç¤ºå¦‚ä¸‹ï¼š
![R-equation](pics/R-equation.png)

é›†åˆ S çš„ mRMR åˆ†æ•°å®šä¹‰ä¸º (D - R)ã€‚ ç›®æ ‡æ˜¯æ‰¾åˆ°å…·æœ‰æœ€å¤§å€¼ (D-R) çš„ç‰¹å¾å­é›†ã€‚ ç„¶è€Œï¼Œåœ¨å®è·µä¸­ï¼Œæˆ‘ä»¬æ‰§è¡Œå¢é‡æœç´¢ï¼ˆä¹Ÿç§°ä¸ºå‰å‘é€‰æ‹©ï¼‰ï¼Œåœ¨æ¯ä¸ªæ­¥éª¤ä¸­ï¼Œæˆ‘ä»¬æ·»åŠ äº§ç”Ÿæœ€å¤§ mRMR çš„ç‰¹å¾ã€‚

è¯¥ç®—æ³•ç”±ç®—æ³•ä½œè€…è‡ªå·±ç”¨ C è¯­è¨€å®ç°ã€‚ æ‚¨å¯ä»¥åœ¨æ­¤å¤„æ‰¾åˆ°åŒ…çš„æºä»£ç ä»¥åŠåŸå§‹è®ºæ–‡ã€‚

åœ¨åç§° pymrmr ä¸Šåˆ›å»ºäº†ä¸€ä¸ªï¼ˆæœªç»´æŠ¤çš„ï¼‰python åŒ…è£…å™¨ã€‚ å¦‚æœ pymrmr å‡ºç°é—®é¢˜ï¼Œæˆ‘å»ºè®®ç›´æ¥è°ƒç”¨ C çº§å‡½æ•°ã€‚

The code below exemplifies the use of pymrmr . Note that the columns of the pandas data-frame should be formatted as described in the C level package (here).

```python
import pandas as pd
import pymrmr
df = pd.read_csv('some_df.csv')
# Pass a dataframe with a predetermined configuration. 
# Check http://home.penglab.com/proj/mRMR/ for the dataset requirements
# dfï¼šæ•°æ®æ–‡ä»¶ï¼ˆpandas.DataFrameï¼‰
    # ç¬¬ä¸€åˆ—è¦æ˜¯æ ‡ç­¾
    # å…¶ä»–åˆ—è¦æ˜¯ç‰¹å¾é‡
    # åˆ—çš„åç§°è¦æ˜¯å­—ç¬¦ä¸²
# å†…éƒ¨é€‰æ‹©æ–¹æ³•ï¼ˆâ€˜MIQâ€™æˆ–â€˜MIDâ€™ï¼‰ï¼ˆstrï¼‰
    # MIQ:äº’ä¿¡æ¯ç†µ
    # MID:äº’ä¿¡æ¯å·®
# Kï¼šè¦é€‰å–çš„ç‰¹å¾æ•°é‡ï¼ˆintï¼‰

pymrmr.mRMR(df, 'MIQ', 10)
Output:

*** This program and the respective minimum Redundancy Maximum Relevance (mRMR)
     algorithm were developed by Hanchuan Peng <hanchuan.peng@gmail.com>for
     the paper
     "Feature selection based on mutual information: criteria of
      max-dependency, max-relevance, and min-redundancy,"
      Hanchuan Peng, Fuhui Long, and Chris Ding,
      IEEE Transactions on Pattern Analysis and Machine Intelligence,
      Vol. 27, No. 8, pp.1226-1238, 2005.
*** MaxRel features ***
 Order    Fea     Name    Score
 1        765     v765    0.375
 2        1423    v1423   0.337
 3        513     v513    0.321
 4        249     v249    0.309
 5        267     v267    0.304
 6        245     v245    0.304
 7        1582    v1582   0.280
 8        897     v897    0.269
 9        1771    v1771   0.269
 10       1772    v1772   0.269
*** mRMR features ***
 Order    Fea     Name    Score
 1        765     v765    0.375
 2        1123    v1123   24.913
 3        1772    v1772   3.984
 4        286     v286    2.280
 5        467     v467    1.979
 6        377     v377    1.768
 7        513     v513    1.803
 8        1325    v1325   1.634
 9        1972    v1972   1.741
 10       1412    v1412   1.689
Out[1]:
 ['v765',
  'v1123',
  'v1772',
  'v286',
  'v467',
  'v377',
  'v513',
  'v1325',
  'v1972',
  'v1412']
```

## Wrapper methods

åŒ…è£…æ–¹æ³•èƒŒåçš„ä¸»è¦æ€æƒ³æ˜¯æœç´¢å“ªä¸€ç»„ç‰¹å¾æœ€é€‚åˆç‰¹å®šçš„åˆ†ç±»å™¨ã€‚ è¿™äº›æ–¹æ³•å¯ä»¥æ€»ç»“å¦‚ä¸‹ï¼Œå¹¶ä¸”åœ¨ä½¿ç”¨çš„æœç´¢ç®—æ³•æ–¹é¢æœ‰æ‰€ä¸åŒã€‚

é€‰æ‹©ä¸€ä¸ªæ€§èƒ½æŒ‡æ ‡ï¼ˆå¯èƒ½æ€§ã€AICã€BICã€F1 åˆ†æ•°ã€å‡†ç¡®åº¦ã€MSEã€MAEâ€¦â€¦ï¼‰ï¼Œè®°ä¸º Mã€‚
é€‰æ‹©ä¸€ä¸ªåˆ†ç±»å™¨/å›å½’å™¨/â€¦ï¼Œåœ¨è¿™é‡Œè®°ä¸º Cã€‚
ä½¿ç”¨ç»™å®šçš„æœç´¢æ–¹æ³•æœç´¢ä¸åŒçš„ç‰¹å¾å­é›†ã€‚ å¯¹äºæ¯ä¸ªå­é›† Sï¼Œæ‰§è¡Œä»¥ä¸‹æ“ä½œï¼š
ä½¿ç”¨ S ä½œä¸ºåˆ†ç±»å™¨çš„ç‰¹å¾ï¼Œä»¥äº¤å‰éªŒè¯æ¨¡å¼è®­ç»ƒå’Œæµ‹è¯• Cï¼›
ä»äº¤å‰éªŒè¯è¿‡ç¨‹ä¸­è·å¾—å¹³å‡åˆ†æ•°ï¼ˆå¯¹äºåº¦é‡ Mï¼‰å¹¶å°†è¯¥åˆ†æ•°åˆ†é…ç»™å­é›† Sï¼›
é€‰æ‹©ä¸€ä¸ªæ–°å­é›†å¹¶é‡åšæ­¥éª¤ aã€‚
Detailing Step 3
Step three leaves unspecified the type which search method will be used. Testing all possible subsets of features is prohibitive (Brute Force selection) in virtually any situation since it would require performing step 3 an exponential number of times (2 to the power of the number of features). Besides the time complexity, with such a large number of possibilities, it would be likely that a certain combination of features performs best simply by random chance, which makes the brute force solution more prone to overfitting.

Search algorithms tend to work well in practice to solve this issue. They tend to achieve a performance close to the brute force solution, with much less time complexity and less chance of overfitting.

Forward selection and Backward selection (aka pruning) are much used in practice, as well as some small variations of their search process.

Backward selection consists of starting with a model with the full number of features and, at each step, removing the feature without which the model has the highest score. Forward selection goes on the opposite way: it starts with an empty set of features and adds the feature that best improves the current score.

Forward/Backward selection are still prone to overfitting, as, usually, scores tend to improve by adding more features. One way to avoid such situation is to use scores that penalize the complexity of the model, such as AIC or BIC.

An illustration of a wrapper method structure is shown below. It is important to note that the feature set is (1) found through a search method and (2) cross-validated on the same classifier it is intended to be used for.


Step three also leaves open the cross-validation parameters. Usually, a k-fold procedure is used. Using a large k, however, introduces extra complexity to the overall wrapper method.

A Python Package for wrapper methods
mlxtend (http://rasbt.github.io/mlxtend/) is a useful package for diverse data science-related tasks. The wrapper methods on this package can be found on SequentialFeatureSelector. It provides Forward and Backward feature selection with some variations.

The package also provides a way to visualize the score as a function of the number of features through the function plot_sequential_feature_selection.

The example below was extracted from the packageâ€™s main page.

from mlxtend.feature_selection import SequentialFeatureSelector as SFS
from mlxtend.plotting import plot_sequential_feature_selection as plot_sfs
from sklearn.linear_model import LinearRegression
from sklearn.datasets import load_boston
boston = load_boston()
X, y = boston.data, boston.target
lr = LinearRegression()
sfs = SFS(lr, 
          k_features=13, 
          forward=True, 
          floating=False, 
          scoring='neg_mean_squared_error',
          cv=10)
sfs = sfs.fit(X, y)
fig = plot_sfs(sfs.get_metric_dict(), kind='std_err')
plt.title('Sequential Forward Selection (w. StdErr)')
plt.grid()
plt.show()

Embedded methods

Training a classifier boils down to an optimization problem, where we try to minimize a function of its parameters (noted here as ğœƒ). This function is known loss function (noted as ğ¿(ğœƒ)).

In a more general framework, we usually want to minimize an objective function that takes into account both the loss function and a penalty (or regularisation)(Î©(ğœƒ)) to the complexity of the model:

obj(ğœƒ)=ğ¿(ğœƒ)+Î©(ğœƒ)

Embedded methods for Linear classifiers
For linear classifiers (e.g. Linear SVM, Logistic Regression), the loss function is noted as :


Where each xÊ² corresponds to one data sample and Wáµ€xÊ² denotes the inner product of the coefficient vector (wâ‚,wâ‚‚,â€¦w_n) with the features in each sample.

For Linear SVM and Logistic Regression the hinge and logistic losses are, respectively:


The two most common penalties for linear classifiers are the L-1 and L-2 penalties:


The higher the value of Î», the stronger the penalty and the optimal objective function will tend to end up in shrinking more and more the coefficients w_i.

The â€œL1â€ penalty is known to create sparse models, which simply means that, it tends to select some features out of the model by making some of the coefficients equal zero during the optimization process.

Another common penalty is L-2. While L-2 shrinks the coefficients and therefore helps avoid overfitting, it does not create sparse models, so it is not suitable as a feature selection technique.

For some linear classifiers (Linear SVM, Logistic Regression), the L-1 penalty can be efficiently used, meaning that there are efficient numerical methods to optimize the resulting objective function. The same is not true for several other classifiers (various Kernel SVM methods, Decision Trees,â€¦). Therefore, different regularisation methods should be used for different classifiers.

An example of Logistic regression with regularisation is shown below, and we can see that the algorithms rule out some of the features as C decreases (think if C as 1/Î»).

import numpy as np
import matplotlib.pyplot as plt
from sklearn.svm import LinearSVC
from sklearn.model_selection import ShuffleSplit
from sklearn.model_selection import GridSearchCV
from sklearn.utils import check_random_state
from sklearn import datasets
from sklearn.linear_model import LogisticRegression
rnd = check_random_state(1)
# set up dataset
n_samples = 3000
n_features = 15
# l1 data (only 5 informative features)
X, y = datasets.make_classification(n_samples=n_samples,
                                        n_features=n_features, n_informative=5,
                                        random_state=1)
cs = np.logspace(-2.3, 0, 50)
coefs = []
for c in cs:
    clf = LogisticRegression(solver='liblinear',C=c,penalty='l1')
    # clf = LinearSVC(C=c,penalty='l1', loss='squared_hinge', dual=False, tol=1e-3)
    
    clf.fit(X,y)
    coefs.append(list(clf.coef_[0]))
    
coefs = np.array(coefs)
plt.figure(figsize=(10,5))
for i,col in enumerate(range(n_features)):
    plt.plot(cs,coefs[:,col])
plt.xscale('log')
plt.title('L1 penalty - Logistic regression')
plt.xlabel('C')
plt.ylabel('Coefficient value')
plt.show()

Feature importances from tree-based models
Another common feature selection technique consists in extracting a feature importance rank from tree base models.

The feature importances are essentially the mean of the individual treesâ€™ improvement in the splitting criterion produced by each variable. In other words, it is how much the score (so-called â€œimpurityâ€ on the decision tree notation) was improved when splitting the tree using that specific variable.

They can be used to rank features and then select a subset of them. However, the feature importances should be used with care, as they suffer from biases and, and presents an unexpected behavior regarding highly correlated features regardless of how strong they are.

As shown in this paper, random forest feature importances are biased towards features with more categories. Besides, if two features are highly correlated, both of their scores largely decrease, regardless of the quality of the features.

Below is an example of how to extract the feature importances from a random forest. Although a regressor, the process would be the same for a classifier.

from sklearn.datasets import load_boston
from sklearn.ensemble import RandomForestRegressor
import numpy as np
boston = load_boston()
X = boston.data
Y = boston.target
feat_names = boston.feature_names 
rf = RandomForestRegressor()
rf.fit(X, Y)
print("Features sorted by their score:")
print(sorted(zip(map(lambda x: round(x, 4), rf.feature_importances_), feat_names), 
             reverse=True))
Out:
Features sorted by their score:
[(0.4334, 'LSTAT'), (0.3709, 'RM'), (0.0805, 'DIS'), (0.0314, 'CRIM'), (0.0225, 'NOX'), (0.0154, 'TAX'), (0.0133, 'PTRATIO'), (0.0115, 'AGE'), (0.011, 'B'), (0.0043, 'INDUS'), (0.0032, 'RAD'), (0.0016, 'CHAS'), (0.0009, 'ZN')]
Extra: main Impurity scores for tree models
As explained above, the â€œimpurityâ€ is a score used by the decision tree algorithm when deciding to split a node. There are many decision tree algorithms (IDR3, C4.5, CART,â€¦), but the general rule is that the variable with which we split a node in the tree is the one that generates the highest improvement on the impurity.

The most common impurities are the Gini Impurity and Entropy. An improvement on the Gini impurity is known as â€œGini importanceâ€ while An improvement on the Entropy is the Information Gain.


SHAP: Reliable feature importances from tree models
(Thanks to 
Henrique Gasparini Fiuza do Nascimento
 for the suggestion!)

SHAP is actually much more than just that. It is an algorithm to provide model explanation out of any predictive model. For tree based models, however, it is specially useful: the authors developed a high speed and exact (not only local) explanation for such models, compatible with XGBoost, LightGBM, CatBoost, and scikit-learn tree models.

I encourage checking out the explanation capabilities provided by SHAP (such as Feature dependance, interaction effects, model monitoringâ€¦). Below, I plot (only) the feature importances output by SHAP, which are more reliable than those output by the original tree model when ranking them for feature selection. This example was extracted from their github page.

import xgboost
import shap
# load JS visualization code to notebook
shap.initjs()
# train XGBoost model
X,y = shap.datasets.boston()
model = xgboost.train({"learning_rate": 0.01}, xgboost.DMatrix(X, label=y), 100)
# explain the model's predictions using SHAP values
# (same syntax works for LightGBM, CatBoost, and scikit-learn models)
explainer = shap.TreeExplainer(model)
shap_values = explainer.shap_values(X)
shap.summary_plot(shap_values, X, plot_type="bar")

Conclusion â€” when to use each method?
Embedded methods are usually very efficient to avoid overfitting and select useful variables. They are also time efficient as they are embedded on the objective function. Their main downside is that they may not be available to the desired classifier.

Wrapper methods tend to work very well in practice. However, they are computationally expensive, specially when dealing hundreds of features. But if you have the computational resources, they are an excellent way to go.

If the feature set is very large (on the order of hundreds or thousands), because filter methods are fast, they can work well as a first stage of selection, to rule out some variables. Subsequently another method can be applied to the already reduced feature set. This is particular useful if you want to create combinations of features, multiplying or dividing them, for example.

References
An Introduction to Variable and Feature Selection

Bias in random forest variable importance measures: Illustrations, sources and a solution

Feature Selection for Classification: A Review

Machine Learning
Feature Selection
Python
Data Science
1.99K


2






## Feature Ranking with the ROC curve


è¿™ä¸ªæŠ€æœ¯æœ€é€‚åˆç”¨äºäºŒåˆ†ç±»ä»»åŠ¡ã€‚å°†è¯¥æŠ€æœ¯ç”¨äºå¤šåˆ†ç±»æ—¶ï¼Œéœ€è¦ä½¿ç”¨microæˆ–macroå¹³å‡ï¼Œæˆ–è€…åŸºäºå‡†åˆ™çš„å¤šæ¯”è¾ƒï¼ˆmultiple comparisonï¼‰ï¼ˆç±»ä¼¼äºpairwise Tukeyâ€™s range testï¼‰


```python
from sklearn.datasets import load_iris
import matplotlib.pyplot as plt
from sklearn.metrics import auc
import numpy as np
# loading dataset
data = load_iris()
X, y = data.data, data.target
y_ = y == 2
plt.figure(figsize=(13,7))
for col in range(X.shape[1]):
    tpr,fpr = [],[]
    for threshold in np.linspace(min(X[:,col]),max(X[:,col]),100):
        detP = X[:,col] < threshold
        tpr.append(sum(detP & y_)/sum(y_))# TP/P, aka recall
        fpr.append(sum(detP & (~y_))/sum((~y_)))# FP/N
        
    if auc(fpr,tpr) < .5:
        aux = tpr
        tpr = fpr
        fpr = aux
    plt.plot(fpr,tpr,label=data.feature_names[col] + ', auc = '\
                           + str(np.round(auc(fpr,tpr),decimals=3)))
plt.title('ROC curve - Iris features')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.legend()
plt.show()
```

![roc_curve](pics/roc_curve.png)

## Multivariate Filter methods


## Reference
[1] [Blog](https://towardsdatascience.com/feature-selection-techniques-for-classification-and-python-tips-for-their-application-10c0ddd7918b)