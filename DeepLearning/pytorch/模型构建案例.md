
# 构建多层LSTM，并添加batchnorm和dropout

```python
import torch
import torch.nn as nn

class MyModel(nn.Module):
    def __init__(self, input_size, hidden_size, num_layers, output_size, dropout_prob=0.2):
        super(MyModel, self).__init__()
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        # nn.ModuleList()用于放置module的list
        self.lstm_layers = nn.ModuleList()
        self.batch_norm_layers = nn.ModuleList()
        self.dropout_layers = nn.ModuleList()
        # 循环，构建list
        for i in range(num_layers):
            if i == 0:
                input_dim = input_size
            else:
                input_dim = hidden_size
            
            self.lstm_layers.append(nn.LSTM(input_dim, hidden_size, batch_first=True))
            self.batch_norm_layers.append(nn.BatchNorm1d(hidden_size))
            self.dropout_layers.append(nn.Dropout(dropout_prob))

        self.fc = nn.Linear(hidden_size, output_size)
    
    def forward(self, x):
        h_n = None
        c_n = None

        for i in range(self.num_layers):
            out, (h_n, c_n) = self.lstm_layers[i](x, (h_n, c_n))
            out = self.batch_norm_layers[i](out)
            out = self.dropout_layers[i](out)

        out = self.fc(out[:, -1, :])
        return out

```